{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom notebook I did for the course \"Fundamentals of Deep Reinforcement Learning\" by LVx, as the original one is not available anymore on edX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 1 : Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating the 3 machines described in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class random_number_gaussian():\n",
    "    \"\"\"\n",
    "    return a random number based on a Gaussian distribution\n",
    "    Input : mean and standard deviation of the Gaussian\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std_dev):\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "    \n",
    "    def pull(self):\n",
    "        return np.random.normal(self.mean, self.std_dev)\n",
    "\n",
    "mach_1 = random_number_gaussian(1, 2) # random values, not the ones in the video pr√©cisely\n",
    "mach_2 = random_number_gaussian(0, 3)\n",
    "mach_3 = random_number_gaussian(3, 5)\n",
    "\n",
    "list_machines = [mach_1, mach_2, mach_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the action space (list of possible actions), the action-value (expected reward for each action) and the count of how many time we pulled each levers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 1, 2] # 3 possible levers to pull\n",
    "Q = [0] * len(a) # the same as Q = [0, 0, 0]\n",
    "N = [0] * len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our agent, he has no memory as we used external variables to store the action value and the number of time we pulled the levers, so we can use a simple function rather than a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(a, Q, epsilon):\n",
    "    \"\"\"\n",
    "    An epsilon greedy agent.\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        action = np.random.choice(a)\n",
    "    else:\n",
    "        action = np.argmax(Q)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's make our agent interact with the environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(a, Q, epsilon, N):\n",
    "    \"\"\"\n",
    "    Make the agent interact with the environment one time\n",
    "    a is the action space, Q the action value and N the number of time each lever was pulled\n",
    "    \"\"\"\n",
    "    action = agent(a, Q, epsilon)\n",
    "    reward = list_machines[action].pull()\n",
    "    N[action] += 1\n",
    "    Q[action] += 1/N[action] * (reward - Q[action])\n",
    "\n",
    "    return Q, N, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "number_interactions = 1000\n",
    "\n",
    "for i in range(number_interactions):\n",
    "    Q, N, r = play(a, Q, epsilon, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see what are the values of Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8230317056615912, 0.07601606257128819, 3.0067971619273424]\n"
     ]
    }
   ],
   "source": [
    "print(Q)\n",
    "# The values are close to the mean value of each machine we defined earlier (1, 0 and 3), so our agent correctly learned which was the best to pull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a decreasing value of epsilon, as said in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005861323928130653\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.99 # initial value (we want to explore first)\n",
    "decay = 0.95\n",
    "number_interactions = 100\n",
    "\n",
    "for i in range(number_interactions):\n",
    "    Q, N, r = play(a, Q, epsilon, N)\n",
    "    epsilon *= decay\n",
    "\n",
    "print(epsilon) # final value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare performances with different values of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final reward for a mainly exploring agent, ie large epsilon : 115.78234113242597\n",
      "final reward for an agent that mainly exploit, but sometime explores : 219.93919757223458\n",
      "final reward for an agent that starts by exploring and then exploit : 265.9679862159174\n"
     ]
    }
   ],
   "source": [
    "# Large value of epsilon = an agent that explore a lot, at the cost of not exploiting a lot what he finds\n",
    "epsilon = 0.8\n",
    "number_interactions = 100\n",
    "a = [0, 1, 2]\n",
    "Q = [0] * len(a)\n",
    "N = [0] * len(a)\n",
    "\n",
    "total_reward = 0\n",
    "for i in range(number_interactions):\n",
    "    Q, N, r = play(a, Q, epsilon, N)\n",
    "    total_reward += r\n",
    "print(\"final reward for a mainly exploring agent, ie large epsilon : \" + str(total_reward))\n",
    "\n",
    "\n",
    "# Relatively small value of epsilon, an agent that exploit and explore sometime\n",
    "epsilon = 0.2\n",
    "number_interactions = 100\n",
    "a = [0, 1, 2]\n",
    "Q = [0] * len(a)\n",
    "N = [0] * len(a)\n",
    "\n",
    "total_reward = 0\n",
    "for i in range(number_interactions):\n",
    "    Q, N, r = play(a, Q, epsilon, N)\n",
    "    total_reward += r\n",
    "print(\"final reward for an agent that mainly exploit, but sometime explores : \" + str(total_reward))\n",
    "\n",
    "# decreasing value : an agent that starts by exploring and then exploiting\n",
    "epsilon = 0.99 # initial value (we want to explore first)\n",
    "decay = 0.95\n",
    "number_interactions = 100\n",
    "a = [0, 1, 2]\n",
    "Q = [0] * len(a)\n",
    "N = [0] * len(a)\n",
    "\n",
    "total_reward = 0\n",
    "for i in range(number_interactions):\n",
    "    Q, N, r = play(a, Q, epsilon, N)\n",
    "    total_reward += r\n",
    "    epsilon *= decay\n",
    "print(\"final reward for an agent that starts by exploring and then exploit : \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best tradeoff in this case is to use a decreasing epsilon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
