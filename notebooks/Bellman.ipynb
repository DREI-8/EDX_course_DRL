{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom notebook I did for the course \"Fundamentals of Deep Reinforcement Learning\" by LVx, as the original one is not available anymore on edX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 2 : The Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to reproduce the environement described at 11:55 in the Bellman equation video, and we will try to found the state-value function for a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![let's add a number to each state](../pictures/shemas.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environement():\n",
    "    \"\"\"implementation of the environement described in the picture above\"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = list(range(8))\n",
    "        self.possible_action = np.array([[1, 2], [3, 4], [4, 5], [6, 4], [3, 5], [4, 7]]) # one line per state, from 0 to 5, and one possible destination per column based on the state\n",
    "        self.rewards = np.array([[-2, 3], [3, 1], [15, 5], [-6, 3], [3, -4], [-4, -2]]) # for each state (one per line) give the reward associated with the action on the previous array\n",
    "        self.current_state = 0\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.possible_action[self.current_state]\n",
    "    \n",
    "    def make_action(self, action):\n",
    "        # reward = self.rewards[self.current_state][np.where(self.possible_action[self.current_state] == action)[0][0]] # we find in the reward table what is the reward for the transition from current state to state \"action\"\n",
    "        self.current_state = action\n",
    "        # return reward\n",
    "    \n",
    "    def get_reward_info(self, action):\n",
    "        reward = self.rewards[self.current_state][np.where(self.possible_action[self.current_state] == action)[0][0]] # we find in the reward table what is the reward for the transition from current state to state \"action\"\n",
    "        return reward\n",
    "    \n",
    "    def check_end(self):\n",
    "        if self.current_state == 6 or self.current_state == 7:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "    \n",
    "    def restart(self):\n",
    "        self.current_state = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have an environement to interact with, let's start making an agent following a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent_class():\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "    \n",
    "    def act(self,possible_action):\n",
    "        if self.policy == \"random_policy\":\n",
    "            return np.random.choice(possible_action)\n",
    "        elif self.policy == \"75left\":\n",
    "            return np.random.choice(possible_action, p=[0.75, 0.25])\n",
    "        elif self.policy == \"75right\":\n",
    "            return np.random.choice(possible_action, p=[0.25, 0.75])\n",
    "    def action_proba(self):\n",
    "        \"\"\"return the probability of choosing action 1 and action 2\"\"\"\n",
    "        if self.policy == \"random_policy\":\n",
    "            return [0.5,0.5]\n",
    "        elif self.policy == \"75left\":\n",
    "            return [0.75, 0.25]\n",
    "        elif self.policy == \"75right\":\n",
    "            return [0.25, 0.75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialise the state-value function randomly (except from the two terminal states, on which we know the value is 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = np.random.randint(0, 10, 6).tolist() + [0, 0] # 6 random numbers for the 6 first states, and two zeros for the terminal states\n",
    "v = [0] * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make ou agent interact with the environement and we will update the state-value mapping at each choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(num_episode, env, agent, v):\n",
    "    for i in range(num_episode):\n",
    "        env.restart()\n",
    "        while env.check_end() == False:\n",
    "            current_state = env.get_current_state()\n",
    "            possible_actions = env.get_actions()\n",
    "            r0 = env.get_reward_info(possible_actions[0]) # reward associated with the choice 1 at current state\n",
    "            r1 = env.get_reward_info(possible_actions[1]) # // choice 2\n",
    "            action_probability = agent.action_proba()     # probability for the agent to choose choice 1 and choice 2\n",
    "\n",
    "            # given that the environement is deterministic, ie : it give the same reward and next state given a certain state and action, the equation can be simplified as :\n",
    "            # the sum over all action probabilities of the associated reward and next state value function (multiplied by the discount factor)\n",
    "            v[current_state] =  (\n",
    "                action_probability[0] * (gamma * v[possible_actions[0]] + r0) + \n",
    "                action_probability[1] * (gamma * v[possible_actions[1]] + r1)\n",
    "            )\n",
    "\n",
    "            action = agent.act(possible_actions)\n",
    "            env.make_action(action)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.496638655462184, -1.444012605042017, 5.880987394957982, -3.409663865546219, -4.243697478991597, -4.909663865546219, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9 # discount factor\n",
    "num_episode = 1000\n",
    "\n",
    "env = environement()\n",
    "agent = agent_class(\"random_policy\")\n",
    "\n",
    "v_final = play(num_episode, env, agent, v)\n",
    "print(v_final)\n",
    "# The results are coherent : for exemple it is much more valuable to be in state 2 rather than 1 because state 2 offers 2 good rewards in comparaison of state 1, and less chances of the max penalty of -6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have notice that we are using the knowledge of both choices reward in order to update the value function, which in a real case is often not possible as the agent will only explore one choice and therefore know only the associated reward, at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus : We can try with a different policy, for exemple we can use one that has 75 % chance of going left, and 25 % of chance of going right. This should make the state 2 even more valuable than in the previous random policy, as there is a greater chance of having the +15 reward on the left (but this state is harder to reach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values when going more often left : [0.5555579555655292, -1.1510491472172353, 9.255627244165169, -4.418536804308797, -2.9712746858168764, -5.5056104129263925, 0, 0]\n",
      "values when going more often right : [3.6648504824955115, -2.807063509874328, 3.772503366247755, -2.903725314183125, -5.412926391382407, -3.7179084380610417, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9 # discount factor\n",
    "num_episode = 1000\n",
    "\n",
    "env = environement()\n",
    "agent = agent_class(\"75left\")\n",
    "\n",
    "v_final = play(num_episode, env, agent, v)\n",
    "print(\"values when going more often left : \" + str(v_final))\n",
    "\n",
    "agent = agent_class(\"75right\")\n",
    "v_final = play(num_episode, env, agent, v)\n",
    "print(\"values when going more often right : \" + str(v_final))\n",
    "\n",
    "# We can see that having a greater chance of going left indeed make more valuable the state 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
